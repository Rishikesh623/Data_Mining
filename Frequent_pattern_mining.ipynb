{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmfe+GKzsCePG+WZeWaSxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishikesh623/Data_Mining/blob/main/Frequent_pattern_mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "r-vrrO7Dp2f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(file_path, id_col, items_col, delimiter=','):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df[items_col] = df[items_col].apply(lambda x: [item.strip() for item in str(x).split(delimiter)])\n",
        "\n",
        "    encoded_df = df[items_col].explode().reset_index()\n",
        "    one_hot = pd.crosstab(encoded_df['index'], encoded_df[items_col])\n",
        "\n",
        "    final_df = df[[id_col]].join(one_hot)\n",
        "    return final_df"
      ],
      "metadata": {
        "id": "rdOF7zy-GXa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence(item,dataset):\n",
        "   total_tuples = dataset.shape[0]\n",
        "   conf  =  0\n",
        "   for i in range(total_tuples):\n",
        "      isPresent = True;\n",
        "      for j in range(len(item)):\n",
        "        if(dataset.iloc[i,dataset.columns.get_loc(item[j])] == 0):\n",
        "          isPresent = False\n",
        "          break\n",
        "      if isPresent:\n",
        "        conf += 1\n",
        "   return conf/total_tuples"
      ],
      "metadata": {
        "id": "tFjv9YYYsxRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_frequent_1_itemsets(df, min_sup):\n",
        "    df = df.drop(columns=[df.columns[0]])\n",
        "    item_counts = df.sum()\n",
        "    frequent_items = [[item] for item, count in item_counts.items() if count >= min_sup]\n",
        "    return frequent_items"
      ],
      "metadata": {
        "id": "94LNi9D6u6x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apriori(dataset, min_sup):\n",
        "    l_1 = find_frequent_1_itemsets(dataset, min_sup)\n",
        "    l = [l_1]\n",
        "    k = 2\n",
        "\n",
        "    while len(l[k - 2]) > 0:\n",
        "        c_k = generate_candidate(l[k - 2])\n",
        "        frequent_k = []\n",
        "\n",
        "        for candidate in c_k:\n",
        "            count = 0\n",
        "            for i in range(len(dataset)):\n",
        "                transaction = set(dataset.columns[dataset.iloc[i] == 1])\n",
        "                if set(candidate).issubset(transaction):\n",
        "                    count += 1\n",
        "\n",
        "            if count >= min_sup:\n",
        "                frequent_k.append(candidate)\n",
        "\n",
        "        l.append(frequent_k)\n",
        "        k += 1\n",
        "\n",
        "    return l"
      ],
      "metadata": {
        "id": "jr_rbLgtt04r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidate(l_k_minus_1):\n",
        "    c_k = []\n",
        "    for i in range(len(l_k_minus_1)):\n",
        "        l1 = l_k_minus_1[i]\n",
        "        for j in range(i + 1, len(l_k_minus_1)):\n",
        "            l2 = l_k_minus_1[j]\n",
        "            if l1[:-1] == l2[:-1] and l1[-1] < l2[-1]:\n",
        "                c = l1 + [l2[-1]]\n",
        "                if not has_infrequent_subset(c, l_k_minus_1):\n",
        "                    c_k.append(c)\n",
        "    return c_k\n",
        "\n",
        "def has_infrequent_subset(candidate, l_k_minus_1):\n",
        "    l_k_minus_1_set = {tuple(itemset) for itemset in l_k_minus_1}\n",
        "    for i in range(len(candidate)):\n",
        "        subset = candidate[:i] + candidate[i+1:]\n",
        "        if tuple(subset) not in l_k_minus_1_set:\n",
        "            return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "KRCklpqgrkxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_subsets(itemset):\n",
        "    n = len(itemset)\n",
        "    subsets = []\n",
        "    for i in range(1, 2**n - 1):\n",
        "        subset = []\n",
        "        for j in range(n):\n",
        "            if (i >> j) & 1:\n",
        "                subset.append(itemset[j])\n",
        "        subsets.append(subset)\n",
        "    return subsets\n",
        "\n",
        "def calculate_support(itemset, dataset):\n",
        "    count = 0\n",
        "    for _, row in dataset.iterrows():\n",
        "        if all(row[item] == 1 for item in itemset):\n",
        "            count += 1\n",
        "    return count / len(dataset)\n",
        "\n",
        "def generate_association_rules(frequent_itemsets, dataset, min_conf=0.5):\n",
        "    rules = []\n",
        "\n",
        "    dataset = dataset.drop(dataset.columns[0], axis=1)\n",
        "\n",
        "    for k_itemset in frequent_itemsets:\n",
        "        for itemset in k_itemset:\n",
        "          if len(itemset) < 2:\n",
        "            continue\n",
        "          itemset_support = calculate_support(itemset, dataset)\n",
        "\n",
        "          for subset in generate_subsets(itemset):\n",
        "              remaining = [item for item in itemset if item not in subset]\n",
        "              if not remaining:\n",
        "                  continue\n",
        "\n",
        "              subset_support = calculate_support(subset, dataset)\n",
        "              remaining_support = calculate_support(remaining, dataset)\n",
        "\n",
        "              if subset_support == 0:\n",
        "                  continue\n",
        "\n",
        "              confidence = itemset_support / subset_support\n",
        "              lift = confidence / remaining_support if remaining_support > 0 else 0\n",
        "\n",
        "              if confidence >= min_conf:\n",
        "                  rules.append({\n",
        "                      'rule': f\"{subset} => {remaining}\",\n",
        "                      'support': round(itemset_support, 4),\n",
        "                      'confidence': round(confidence, 4),\n",
        "                      'lift': round(lift, 4)\n",
        "                })\n",
        "\n",
        "    return rules\n",
        "\n",
        "def print_rules(rules):\n",
        "    print(\"RULE\\t\\t\\tSUPPORT\\tCONFIDENCE\\tLIFT\")\n",
        "    for r in rules:\n",
        "        print(f\"{r['rule']}\\t{r['support']}\\t{r['confidence']}\\t{r['lift']}\")\n"
      ],
      "metadata": {
        "id": "iBkOQwB8vdQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve(preprocessed_dataset,min_sup,min_conf):\n",
        "  frequent_itemsets = apriori(preprocessed_dataset,min_sup)\n",
        "\n",
        "  rules = generate_association_rules(frequent_itemsets,preprocessed_dataset,min_conf)\n",
        "\n",
        "  print_rules(rules)"
      ],
      "metadata": {
        "id": "aR6foCR6Uapp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_df = preprocess_dataset('groceries.csv', 'Transaction ID', 'Items')\n",
        "transactions_df.to_csv('transactions_onehot.csv', index=False)\n",
        "print(transactions_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRF03Vb1xZXG",
        "outputId": "31d798dc-36f6-4fc1-c41d-efb5b50fe8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Transaction ID  banana  biscuit  bread  butter  cereal  cheese  coffee  \\\n",
            "0           T001       0        0      1       1       0       0       0   \n",
            "1           T002       0        0      1       0       0       0       0   \n",
            "2           T003       0        0      0       1       0       0       0   \n",
            "3           T004       0        0      1       0       0       0       0   \n",
            "4           T005       0        0      0       0       1       0       0   \n",
            "\n",
            "   crackers  eggs  jam  milk  sugar  \n",
            "0         0     0    0     1      0  \n",
            "1         0     1    0     0      0  \n",
            "2         0     0    0     1      0  \n",
            "3         0     0    1     0      0  \n",
            "4         0     0    0     1      0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(find_frequent_1_itemsets(transactions_df,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0khX5sYNBaX",
        "outputId": "7aaa69b2-4ecf-4850-dd98-c85ff743cc9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['banana'], ['biscuit'], ['bread'], ['butter'], ['cereal'], ['cheese'], ['coffee'], ['crackers'], ['eggs'], ['jam'], ['milk'], ['sugar']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transaction csv\n",
        "solve(transactions_df,2,0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zdPXjqYRjor",
        "outputId": "27da6fdb-6203-4c9a-f53b-fb7db64e906c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RULE\t\t\tSUPPORT\tCONFIDENCE\tLIFT\n",
            "['banana'] => ['bread']\t0.0645\t0.5\t1.0333\n",
            "['banana'] => ['cereal']\t0.0645\t0.5\t1.9375\n",
            "['banana'] => ['milk']\t0.0645\t0.5\t1.2917\n",
            "['butter'] => ['bread']\t0.1935\t0.6\t1.24\n",
            "['eggs'] => ['bread']\t0.129\t0.5714\t1.181\n",
            "['jam'] => ['bread']\t0.0645\t1.0\t2.0667\n",
            "['butter'] => ['milk']\t0.1613\t0.5\t1.2917\n",
            "['cereal'] => ['milk']\t0.129\t0.5\t1.2917\n",
            "['bread', 'eggs'] => ['butter']\t0.0645\t0.5\t1.55\n",
            "['butter', 'eggs'] => ['bread']\t0.0645\t0.6667\t1.3778\n",
            "['bread', 'butter'] => ['milk']\t0.0968\t0.5\t1.2917\n",
            "['bread', 'milk'] => ['butter']\t0.0968\t0.6\t1.86\n",
            "['butter', 'milk'] => ['bread']\t0.0968\t0.6\t1.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clickstream dataset\n",
        "clickstream_df = preprocess_dataset('clickstream.csv', 'Session ID', 'Viewed Products')\n",
        "solve(clickstream_df,3,0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QB0Fwl53RlHE",
        "outputId": "f239414d-3b5f-41b1-9138-730849055c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RULE\t\t\tSUPPORT\tCONFIDENCE\tLIFT\n",
            "['P001'] => ['P002']\t0.2333\t0.5\t1.0714\n",
            "['P002'] => ['P001']\t0.2333\t0.5\t1.0714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MovieRatings dataset\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_movie_ratings(file_path, id_col, movie_col, rating_col, min_rating=4):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df = df[df[rating_col] >= min_rating]\n",
        "    grouped_movies = df.groupby(id_col)[movie_col].apply(list).reset_index()\n",
        "    grouped_movies.columns = [id_col, 'Movie List']\n",
        "\n",
        "    return grouped_movies\n",
        "\n",
        "movie_df = preprocess_movie_ratings('moviesrating.csv', 'User ID', 'Movie ID', 'Rating', min_rating=4)\n",
        "\n",
        "movie_df_onehot = pd.DataFrame(columns=['User ID'] + list(set([item for sublist in movie_df['Movie List'] for item in sublist])))\n",
        "\n",
        "for index, row in movie_df.iterrows():\n",
        "    movie_df_onehot.loc[index, 'User ID'] = row['User ID']\n",
        "    for movie in row['Movie List']:\n",
        "        movie_df_onehot.loc[index, movie] = 1\n",
        "\n",
        "movie_df_onehot = movie_df_onehot.fillna(0)\n",
        "\n",
        "print(movie_df_onehot.head())\n",
        "\n",
        "\n",
        "solve(movie_df_onehot, 1, 0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXuI1wbdVnmp",
        "outputId": "c6f532dd-37a3-4fb9-feae-ccab31e13080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  User ID  M003  M007  M005  M002  M001  M006  M004\n",
            "0    U001     0     0     0     1     1     0     0\n",
            "1    U002     1     0     0     0     0     0     1\n",
            "2    U003     1     0     0     0     0     0     0\n",
            "3    U004     0     0     1     1     0     0     0\n",
            "4    U005     0     0     0     1     1     0     0\n",
            "RULE\t\t\tSUPPORT\tCONFIDENCE\tLIFT\n",
            "['M004'] => ['M003']\t0.0667\t0.5\t1.875\n",
            "['M004'] => ['M001']\t0.0667\t0.5\t1.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-180-b8adfd144dea>:22: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  movie_df_onehot = movie_df_onehot.fillna(0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SupermarketTransactions dataset\n",
        "supermarket_df = preprocess_dataset('marketTransaction.csv', 'Transaction ID', 'Item List')\n",
        "solve(supermarket_df,1,0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg5kW3YnVolp",
        "outputId": "488dc0c7-bac3-4ddc-a20c-51bf8dfb6c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RULE\t\t\tSUPPORT\tCONFIDENCE\tLIFT\n",
            "['cheese', 'eggs'] => ['bread']\t0.0333\t1.0\t2.0\n",
            "['cheese', 'jam'] => ['milk']\t0.0333\t1.0\t2.1429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BookstorePurchases dataset\n",
        "bookstore_df = preprocess_dataset('bookstorepurchases.csv', 'Customer ID', 'Books Purchased')\n",
        "solve(bookstore_df,3,0.6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f4AMxorVppg",
        "outputId": "239e793e-279b-47ba-dd5d-8cbbb5ddd811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RULE\t\t\tSUPPORT\tCONFIDENCE\tLIFT\n",
            "['Mystery'] => ['Fiction']\t0.2667\t0.6667\t1.1111\n",
            "['Non-Fiction'] => ['Romance']\t0.2667\t0.6667\t1.1111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SocialMediaEngagement dataset\n",
        "social_df = preprocess_dataset('socialmediaengagement.csv', 'User ID', 'Post Type')\n",
        "solve(social_df,3,0.4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLh_3O6hVqdX",
        "outputId": "f9b366af-4fd0-4322-9649-080ee5b01e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RULE\t\t\tSUPPORT\tCONFIDENCE\tLIFT\n",
            "['memes'] => ['news']\t0.2667\t0.4706\t0.7843\n",
            "['news'] => ['memes']\t0.2667\t0.4444\t0.7843\n",
            "['memes'] => ['videos']\t0.3333\t0.5882\t0.9804\n",
            "['videos'] => ['memes']\t0.3333\t0.5556\t0.9804\n",
            "['news'] => ['videos']\t0.2667\t0.4444\t0.7407\n",
            "['videos'] => ['news']\t0.2667\t0.4444\t0.7407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  RestaurantOrders dataset\n",
        "restaurant_df = preprocess_dataset('restaurantOrders.csv', 'Order ID', 'Items Ordered')\n",
        "solve(restaurant_df,0.05,0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6zR8D67Vr-g",
        "outputId": "a50d44de-d09c-444c-efa7-6757d0134591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RULE\t\t\tSUPPORT\tCONFIDENCE\tLIFT\n",
            "['Fries'] => ['Burger']\t0.2258\t0.7\t1.8083\n",
            "['Garlic Bread'] => ['Pasta']\t0.0968\t0.75\t3.3214\n",
            "['Mashed Potatoes'] => ['Steak']\t0.0645\t1.0\t6.2\n",
            "['Soup'] => ['Salad']\t0.0323\t1.0\t5.1667\n",
            "['Fries', 'Milkshake'] => ['Burger']\t0.0323\t1.0\t2.5833\n",
            "['Garlic Bread', 'Salad'] => ['Pasta']\t0.0323\t1.0\t4.4286\n",
            "['Garlic Bread', 'Soda'] => ['Pasta']\t0.0323\t1.0\t4.4286\n",
            "['Pasta', 'Soda'] => ['Garlic Bread']\t0.0323\t1.0\t7.75\n",
            "['Pasta', 'Water'] => ['Salad']\t0.0323\t1.0\t5.1667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibfTP35GYu_T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}